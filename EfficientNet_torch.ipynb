{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q timm torchmetrics albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os, glob, random, cv2\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from copy import deepcopy\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam, RAdam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torchvision.transforms as transforms\n",
    "import timm\n",
    "from torchmetrics.classification import MultilabelAUROC\n",
    "\n",
    "# Other utilities\n",
    "from albumentations import Compose, VerticalFlip, HorizontalFlip, Rotate, GridDistortion\n",
    "from albumentations.pytorch import ToTensorV2 # Important for PyTorch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "def set_seed(seed=10):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(10)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "test_imgs_dir = '../input/understanding_cloud_organization/test_images/'\n",
    "train_imgs_dir = '../input/understanding_cloud_organization/train_images/'\n",
    "num_cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../input/understanding_cloud_organization/train.csv')\n",
    "train_df = train_df[~train_df['EncodedPixels'].isnull()]\n",
    "train_df['Image'] = train_df['Image_Label'].map(lambda x: x.split('_')[0])\n",
    "train_df['Class'] = train_df['Image_Label'].map(lambda x: x.split('_')[1])\n",
    "classes = train_df['Class'].unique()\n",
    "train_df = train_df.groupby('Image')['Class'].agg(set).reset_index()\n",
    "for class_name in classes:\n",
    "    train_df[class_name] = train_df['Class'].map(lambda x: 1 if class_name in x else 0)\n",
    "\n",
    "# Dictionary for fast access\n",
    "img_2_ohe_vector = {img:vec for img, vec in zip(train_df['Image'], train_df.iloc[:, 2:].values)}\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "train_imgs, val_imgs = train_test_split(train_df['Image'].values,\n",
    "                                        test_size = 0.2,\n",
    "                                        stratify = train_df['Class'].map(lambda x: str(sorted(list(x)))),\n",
    "                                        random_state = 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class CloudDataset(Dataset):\n",
    "    def __init__(self, images_list, folder_imgs=train_imgs_dir,\n",
    "                 transform=None, resized_height=260, resized_width=260, is_test=False):\n",
    "        self.images_list = deepcopy(images_list)\n",
    "        self.folder_imgs = folder_imgs\n",
    "        self.transform = transform\n",
    "        self.resized_height = resized_height\n",
    "        self.resized_width = resized_width\n",
    "        self.is_test = is_test\n",
    "        self.labels = [img_2_ohe_vector[img] for img in self.images_list] if not is_test else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.images_list[idx]\n",
    "        path = os.path.join(self.folder_imgs, image_name)\n",
    "        img = cv2.imread(path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (self.resized_width, self.resized_height))\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=img)\n",
    "            img = augmented['image']\n",
    "\n",
    "        # PyTorch expects C, H, W format\n",
    "        img = img.astype(np.float32) / 255.0\n",
    "        img = img.transpose(2, 0, 1)\n",
    "        \n",
    "        img_tensor = torch.tensor(img, dtype=torch.float32)\n",
    "\n",
    "        if self.is_test:\n",
    "            return img_tensor\n",
    "        else:\n",
    "            label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "            return img_tensor, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "albumentations_train = Compose([\n",
    "    VerticalFlip(p=0.5),\n",
    "    HorizontalFlip(p=0.5),\n",
    "    Rotate(limit=20, p=0.5),\n",
    "    GridDistortion(p=0.5)\n",
    "], p=1)\n",
    "\n",
    "# Create Datasets\n",
    "train_dataset = CloudDataset(train_imgs, transform=albumentations_train)\n",
    "val_dataset = CloudDataset(val_imgs) # No augmentation for validation\n",
    "\n",
    "# Create DataLoaders\n",
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_cores)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def dice_loss(y_pred, y_true, smooth=1.):\n",
    "    y_pred = y_pred.sigmoid() # Apply sigmoid since we use BCEWithLogitsLoss\n",
    "    y_true_f = y_true.flatten(1)\n",
    "    y_pred_f = y_pred.flatten(1)\n",
    "    intersection = (y_true_f * y_pred_f).sum(1)\n",
    "    score = (2. * intersection + smooth) / (y_true_f.sum(1) + y_pred_f.sum(1) + smooth)\n",
    "    return 1. - score.mean()\n",
    "\n",
    "def bce_dice_loss(y_pred, y_true):\n",
    "    # We use BCEWithLogitsLoss for better numerical stability\n",
    "    bce = nn.BCEWithLogitsLoss()\n",
    "    return bce(y_pred, y_true) + dice_loss(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def get_model(model_name='efficientnet_b0', num_classes=4, pretrained=True):\n",
    "    model = timm.create_model(model_name, pretrained=pretrained, num_classes=num_classes)\n",
    "    return model\n",
    "\n",
    "# Instantiate the model and select device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = get_model()\n",
    "model.to(device)\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 1e-3\n",
    "CHECKPOINTS_PATH = 'checkpoints/'\n",
    "if not os.path.exists(CHECKPOINTS_PATH):\n",
    "    os.makedirs(CHECKPOINTS_PATH)\n",
    "\n",
    "# Setup optimizer, loss, and scheduler\n",
    "optimizer = RAdam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = bce_dice_loss\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3)\n",
    "\n",
    "# For metrics\n",
    "pr_auc_metric = MultilabelAUROC(num_labels=4, average=\"macro\", thresholds=None)\n",
    "\n",
    "# For plotting and tracking\n",
    "history = {\n",
    "    'train_loss': [], 'val_loss': [],\n",
    "    'train_pr_auc': [], 'val_pr_auc': []\n",
    "}\n",
    "best_pr_auc = -float('inf')\n",
    "early_stopping_patience = 5\n",
    "patience_counter = 0\n",
    "\n",
    "# --- Main Training Loop ---\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"--- Epoch {epoch+1}/{EPOCHS} ---\")\n",
    "    \n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for images, labels in tqdm(train_loader, desc=\"Training\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    history['train_loss'].append(avg_train_loss)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, desc=\"Validation\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            # Store predictions and labels for metrics\n",
    "            all_preds.append(outputs.sigmoid().cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    history['val_loss'].append(avg_val_loss)\n",
    "\n",
    "    # Calculate PR-AUC\n",
    "    val_preds = torch.cat(all_preds)\n",
    "    val_labels = torch.cat(all_labels).int() # AUROC metric expects integer labels\n",
    "    \n",
    "    pr_auc_val = pr_auc_metric(val_preds, val_labels).item()\n",
    "    history['val_pr_auc'].append(pr_auc_val)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}: Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val PR-AUC: {pr_auc_val:.4f}\")\n",
    "    \n",
    "    # LR Scheduler\n",
    "    scheduler.step(pr_auc_val)\n",
    "\n",
    "    # Model Checkpointing\n",
    "    if pr_auc_val > best_pr_auc:\n",
    "        print(f\"Validation PR-AUC improved from {best_pr_auc:.4f} to {pr_auc_val:.4f}. Saving model...\")\n",
    "        best_pr_auc = pr_auc_val\n",
    "        patience_counter = 0 # Reset patience\n",
    "        torch.save(model.state_dict(), os.path.join(CHECKPOINTS_PATH, 'best_model.pth'))\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    # Early Stopping\n",
    "    if patience_counter >= early_stopping_patience:\n",
    "        print(f\"Early stopping triggered after {early_stopping_patience} epochs with no improvement.\")\n",
    "        break\n",
    "        \n",
    "# Load the best model for inference\n",
    "model.load_state_dict(torch.load(os.path.join(CHECKPOINTS_PATH, 'best_model.pth')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "fs = 10\n",
    "fnt1 = 12\n",
    "fnt2 = 15\n",
    "\n",
    "plt.figure(figsize=(fs, fs))\n",
    "plt.plot(history['val_pr_auc'])\n",
    "plt.xlabel('Epoch', fontsize=fnt1)\n",
    "plt.ylabel('Mean PR-AUC', fontsize=fnt1)\n",
    "plt.legend(['Validation'])\n",
    "plt.title('Validation PR-AUC', fontsize=fnt2)\n",
    "plt.savefig('pr_auc_hist.png')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(fs, fs))\n",
    "plt.plot(history['train_loss'])\n",
    "plt.plot(history['val_loss'])\n",
    "plt.xlabel('Epoch', fontsize=fnt1)\n",
    "plt.ylabel('Loss (BCE + Dice)', fontsize=fnt1)\n",
    "plt.legend(['Train', 'Validation'])\n",
    "plt.title('Training & Validation Loss', fontsize=fnt2)\n",
    "plt.savefig('loss_hist.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Prediction function\n",
    "def predict(model, loader, device):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for images in tqdm(loader, desc=\"Predicting\"):\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            all_predictions.append(outputs.sigmoid().cpu().numpy())\n",
    "    return np.vstack(all_predictions)\n",
    "\n",
    "# Create test dataloader\n",
    "test_image_list = os.listdir(test_imgs_dir)\n",
    "test_dataset = CloudDataset(images_list=test_image_list, folder_imgs=test_imgs_dir, is_test=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_cores)\n",
    "\n",
    "# Get predictions\n",
    "y_pred_test = predict(model, test_loader, device)\n",
    "\n",
    "# Load validation predictions to find thresholds\n",
    "val_pred_for_threshold = predict(model, val_loader, device)\n",
    "y_true = np.array([label.numpy() for _, label in val_dataset])\n",
    "\n",
    "\n",
    "# Thresholding logic (unchanged, just uses new predictions)\n",
    "class_names = ['Fish', 'Flower', 'Sugar', 'Gravel']\n",
    "recall_thresholds = {}\n",
    "precision_thresholds = {}\n",
    "\n",
    "for i, class_name in enumerate(class_names):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true[:, i], val_pred_for_threshold[:, i])\n",
    "    # A simple thresholding strategy: find threshold for a given recall\n",
    "    target_recall = 0.94\n",
    "    valid_indices = np.where(recall >= target_recall)[0]\n",
    "    if len(valid_indices) > 0:\n",
    "        # threshold is not returned for the last value, so we need to handle that\n",
    "        if valid_indices[-1] < len(thresholds):\n",
    "            best_threshold = thresholds[valid_indices[-1]]\n",
    "        else: # if the best recall is the last one\n",
    "             best_threshold = 0.99\n",
    "    else:\n",
    "        best_threshold = 0.5 # Default fallback\n",
    "    recall_thresholds[class_name] = best_threshold\n",
    "    print(f\"Threshold for {class_name} at recall > {target_recall}: {recall_thresholds[class_name]:.3f}\")\n",
    "\n",
    "# SUBMISSION\n",
    "image_labels_empty = set()\n",
    "for i, (img, predictions) in enumerate(zip(test_image_list, y_pred_test)):\n",
    "    for class_i, class_name in enumerate(class_names):\n",
    "        if predictions[class_i] < recall_thresholds[class_name]:\n",
    "            image_labels_empty.add(f'{img}_{class_name}')\n",
    "\n",
    "# Assuming segmentation results are from another model/source\n",
    "# If you don't have this file, you'll need to create a placeholder or generate it\n",
    "try:\n",
    "    submission = pd.read_csv('../input/densenet201cloudy/densenet201.csv')\n",
    "    predictions_nonempty = set(submission.loc[~submission['EncodedPixels'].isnull(), 'Image_Label'].values)\n",
    "    print(f'{len(image_labels_empty.intersection(predictions_nonempty))} masks would be removed')\n",
    "    submission.loc[submission['Image_Label'].isin(image_labels_empty), 'EncodedPixels'] = np.nan\n",
    "    submission.to_csv('submission.csv', index=None)\n",
    "    print(\"Submission file created.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Warning: '../input/densenet201cloudy/densenet201.csv' not found.\")\n",
    "    print(\"Classifier predictions have been made, but submission file cannot be post-processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "007f09756431484e959f31e49feb5061": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_101c6e687955408aa21203427ae57c0e",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_2372eecd16d0445d8e490bf17fdfcf87",
       "value": 1
      }
     },
     "101c6e687955408aa21203427ae57c0e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2372eecd16d0445d8e490bf17fdfcf87": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "4003f5b770e841d193aca3ac46f6f4af": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "52a297c38a334d039d9c5a733daa403e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_007f09756431484e959f31e49feb5061",
        "IPY_MODEL_9ce268a1465940298e231f34e9f1dd81"
       ],
       "layout": "IPY_MODEL_6ac97c65b428480b8fc11d92fb6f25ab"
      }
     },
     "6ac97c65b428480b8fc11d92fb6f25ab": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "89173705a8d24d258f13f8c208525ae3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9ce268a1465940298e231f34e9f1dd81": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_89173705a8d24d258f13f8c208525ae3",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_4003f5b770e841d193aca3ac46f6f4af",
       "value": " 4/? [00:00&lt;00:00, 32.20it/s]"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
